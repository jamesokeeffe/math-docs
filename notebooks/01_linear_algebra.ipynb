{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Linear Algebra - Interactive Learning\n",
    "\n",
    "This notebook provides hands-on exploration of linear algebra concepts fundamental to machine learning.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Visualize vector operations and matrix transformations\n",
    "- Understand eigenvalues and eigenvectors geometrically\n",
    "- Implement and understand PCA\n",
    "- See how linear algebra powers neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector Operations and Geometry\n",
    "\n",
    "Let's start by visualizing basic vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vectors_2d(vectors, labels=None, colors=None, title=\"Vector Operations\"):\n",
    "    \"\"\"Plot 2D vectors from origin.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = [f'v{i+1}' for i in range(len(vectors))]\n",
    "    \n",
    "    for i, (vec, label, color) in enumerate(zip(vectors, labels, colors)):\n",
    "        ax.quiver(0, 0, vec[0], vec[1], \n",
    "                 angles='xy', scale_units='xy', scale=1,\n",
    "                 color=color, width=0.006, label=label)\n",
    "        \n",
    "        # Add vector endpoint\n",
    "        ax.plot(vec[0], vec[1], 'o', color=color, markersize=8)\n",
    "        \n",
    "        # Add text label\n",
    "        ax.text(vec[0]*1.1, vec[1]*1.1, label, fontsize=12, color=color)\n",
    "    \n",
    "    # Set equal aspect ratio and grid\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Set limits\n",
    "    all_coords = np.array(vectors).flatten()\n",
    "    max_coord = np.max(np.abs(all_coords)) * 1.2\n",
    "    ax.set_xlim(-max_coord, max_coord)\n",
    "    ax.set_ylim(-max_coord, max_coord)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example vectors\n",
    "u = np.array([3, 2])\n",
    "v = np.array([1, 4])\n",
    "u_plus_v = u + v\n",
    "u_minus_v = u - v\n",
    "\n",
    "print(f\"u = {u}\")\n",
    "print(f\"v = {v}\")\n",
    "print(f\"u + v = {u_plus_v}\")\n",
    "print(f\"u - v = {u_minus_v}\")\n",
    "\n",
    "plot_vectors_2d([u, v, u_plus_v, u_minus_v], \n",
    "                ['u', 'v', 'u+v', 'u-v'],\n",
    "                ['red', 'blue', 'green', 'purple'],\n",
    "                'Vector Addition and Subtraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive: Dot Product and Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dot_product(u, v):\n",
    "    \"\"\"Analyze dot product and angle between vectors.\"\"\"\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    \n",
    "    cos_theta = dot_product / (norm_u * norm_v)\n",
    "    theta_rad = np.arccos(np.clip(cos_theta, -1, 1))  # Clip to handle numerical errors\n",
    "    theta_deg = np.degrees(theta_rad)\n",
    "    \n",
    "    print(f\"Vector u: {u}\")\n",
    "    print(f\"Vector v: {v}\")\n",
    "    print(f\"||u|| = {norm_u:.3f}\")\n",
    "    print(f\"||v|| = {norm_v:.3f}\")\n",
    "    print(f\"u Â· v = {dot_product:.3f}\")\n",
    "    print(f\"cos(Î¸) = {cos_theta:.3f}\")\n",
    "    print(f\"Î¸ = {theta_deg:.1f}Â°\")\n",
    "    \n",
    "    if abs(cos_theta) < 1e-10:\n",
    "        print(\"Vectors are ORTHOGONAL (perpendicular)\")\n",
    "    elif cos_theta > 0:\n",
    "        print(\"Vectors point in SIMILAR directions\")\n",
    "    else:\n",
    "        print(\"Vectors point in OPPOSITE directions\")\n",
    "    \n",
    "    # Visualize\n",
    "    plot_vectors_2d([u, v], ['u', 'v'], ['red', 'blue'], \n",
    "                   f'Dot Product = {dot_product:.2f}, Angle = {theta_deg:.1f}Â°')\n",
    "\n",
    "# Test different vector pairs\n",
    "print(\"=== Example 1: Acute angle ===\")\n",
    "analyze_dot_product(np.array([3, 2]), np.array([2, 3]))\n",
    "\n",
    "print(\"\\n=== Example 2: Obtuse angle ===\")\n",
    "analyze_dot_product(np.array([3, 2]), np.array([-1, 3]))\n",
    "\n",
    "print(\"\\n=== Example 3: Orthogonal vectors ===\")\n",
    "analyze_dot_product(np.array([1, 0]), np.array([0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix Transformations\n",
    "\n",
    "Matrices represent linear transformations. Let's visualize how they transform shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_transformation(matrix, title=\"Matrix Transformation\"):\n",
    "    \"\"\"Visualize how a matrix transforms a unit square and circle.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original shapes\n",
    "    # Unit square vertices\n",
    "    square = np.array([[0, 1, 1, 0, 0],\n",
    "                       [0, 0, 1, 1, 0]])\n",
    "    \n",
    "    # Unit circle\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    circle = np.array([np.cos(theta), np.sin(theta)])\n",
    "    \n",
    "    # Grid lines\n",
    "    grid_x = np.array([[-2, 2], [-2, 2], [-1, 1], [-1, 1], [0, 0], [0, 0]])\n",
    "    grid_y = np.array([[-1, -1], [1, 1], [-2, 2], [-2, 2], [-2, 2], [-2, 2]])\n",
    "    \n",
    "    # Transform shapes\n",
    "    square_transformed = matrix @ square\n",
    "    circle_transformed = matrix @ circle\n",
    "    grid_transformed = matrix @ np.vstack([grid_x, grid_y])\n",
    "    \n",
    "    # Plot original\n",
    "    axes[0].plot(square[0], square[1], 'b-', linewidth=2, label='Unit Square')\n",
    "    axes[0].plot(circle[0], circle[1], 'r-', linewidth=2, label='Unit Circle')\n",
    "    for i in range(0, len(grid_x), 2):\n",
    "        axes[0].plot(grid_x[i], grid_y[i], 'k-', alpha=0.3)\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xlim(-3, 3)\n",
    "    axes[0].set_ylim(-3, 3)\n",
    "    \n",
    "    # Plot transformed\n",
    "    axes[1].plot(square_transformed[0], square_transformed[1], 'b-', linewidth=2, label='Transformed Square')\n",
    "    axes[1].plot(circle_transformed[0], circle_transformed[1], 'r-', linewidth=2, label='Transformed Circle')\n",
    "    for i in range(0, grid_transformed.shape[1], 2):\n",
    "        axes[1].plot(grid_transformed[0, i:i+2], grid_transformed[1, i:i+2], 'k-', alpha=0.3)\n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_title('Transformed')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Auto-scale for transformed plot\n",
    "    all_coords = np.concatenate([square_transformed.flatten(), circle_transformed.flatten()])\n",
    "    max_coord = np.max(np.abs(all_coords)) * 1.1\n",
    "    axes[1].set_xlim(-max_coord, max_coord)\n",
    "    axes[1].set_ylim(-max_coord, max_coord)\n",
    "    \n",
    "    # Plot both overlaid\n",
    "    axes[2].plot(square[0], square[1], 'b--', alpha=0.5, linewidth=2, label='Original')\n",
    "    axes[2].plot(circle[0], circle[1], 'r--', alpha=0.5, linewidth=2)\n",
    "    axes[2].plot(square_transformed[0], square_transformed[1], 'b-', linewidth=2, label='Transformed')\n",
    "    axes[2].plot(circle_transformed[0], circle_transformed[1], 'r-', linewidth=2)\n",
    "    axes[2].set_aspect('equal')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_title('Overlay')\n",
    "    axes[2].legend()\n",
    "    axes[2].set_xlim(-max_coord, max_coord)\n",
    "    axes[2].set_ylim(-max_coord, max_coord)\n",
    "    \n",
    "    plt.suptitle(f'{title}\\nMatrix: {matrix}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print matrix properties\n",
    "    det = np.linalg.det(matrix)\n",
    "    print(f\"Determinant: {det:.3f}\")\n",
    "    if abs(det) < 1e-10:\n",
    "        print(\"Matrix is SINGULAR (not invertible)\")\n",
    "    elif det < 0:\n",
    "        print(\"Matrix FLIPS orientation (negative determinant)\")\n",
    "    else:\n",
    "        print(\"Matrix PRESERVES orientation (positive determinant)\")\n",
    "    print(f\"Area scaling factor: {abs(det):.3f}\")\n",
    "\n",
    "# Test different transformations\n",
    "print(\"=== Scaling Matrix ===\")\n",
    "scaling = np.array([[2, 0],\n",
    "                    [0, 0.5]])\n",
    "visualize_transformation(scaling, \"Scaling Transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Rotation Matrix ===\")\n",
    "angle = np.pi/4  # 45 degrees\n",
    "rotation = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                     [np.sin(angle), np.cos(angle)]])\n",
    "visualize_transformation(rotation, \"45Â° Rotation\")\n",
    "\n",
    "print(\"\\n=== Shear Matrix ===\")\n",
    "shear = np.array([[1, 1],\n",
    "                  [0, 1]])\n",
    "visualize_transformation(shear, \"Horizontal Shear\")\n",
    "\n",
    "print(\"\\n=== Reflection Matrix ===\")\n",
    "reflection = np.array([[1, 0],\n",
    "                       [0, -1]])\n",
    "visualize_transformation(reflection, \"Reflection across X-axis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Eigenvalues and Eigenvectors\n",
    "\n",
    "Eigenvectors are special directions that matrices only stretch (don't rotate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_eigenvectors(matrix, title=\"Eigenanalysis\"):\n",
    "    \"\"\"Visualize eigenvectors and their effect.\"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Original vectors and eigenvectors\n",
    "    colors = ['red', 'blue']\n",
    "    for i, (val, vec, color) in enumerate(zip(eigenvalues, eigenvectors.T, colors)):\n",
    "        if np.isreal(val) and np.isreal(vec).all():\n",
    "            vec = np.real(vec)\n",
    "            val = np.real(val)\n",
    "            \n",
    "            # Plot eigenvector\n",
    "            axes[0].quiver(0, 0, vec[0], vec[1], \n",
    "                          angles='xy', scale_units='xy', scale=1,\n",
    "                          color=color, width=0.008, \n",
    "                          label=f'Î»={val:.2f}')\n",
    "            \n",
    "            # Plot the line through eigenvector\n",
    "            t = np.linspace(-3, 3, 100)\n",
    "            line_x = t * vec[0]\n",
    "            line_y = t * vec[1]\n",
    "            axes[0].plot(line_x, line_y, '--', color=color, alpha=0.5)\n",
    "    \n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim(-3, 3)\n",
    "    axes[0].set_ylim(-3, 3)\n",
    "    axes[0].set_title('Eigenvectors')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot 2: Unit circle transformation\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    circle = np.array([np.cos(theta), np.sin(theta)])\n",
    "    circle_transformed = matrix @ circle\n",
    "    \n",
    "    axes[1].plot(circle[0], circle[1], 'b--', alpha=0.5, label='Unit Circle')\n",
    "    axes[1].plot(circle_transformed[0], circle_transformed[1], 'r-', linewidth=2, label='Transformed')\n",
    "    \n",
    "    # Add eigenvector directions\n",
    "    for i, (val, vec, color) in enumerate(zip(eigenvalues, eigenvectors.T, colors)):\n",
    "        if np.isreal(val) and np.isreal(vec).all():\n",
    "            vec = np.real(vec)\n",
    "            val = np.real(val)\n",
    "            \n",
    "            # Show how eigenvector gets stretched\n",
    "            original_vec = vec\n",
    "            transformed_vec = matrix @ vec\n",
    "            \n",
    "            axes[1].arrow(0, 0, original_vec[0], original_vec[1],\n",
    "                         head_width=0.1, head_length=0.1, fc=color, ec=color,\n",
    "                         alpha=0.5, width=0.02)\n",
    "            axes[1].arrow(0, 0, transformed_vec[0], transformed_vec[1],\n",
    "                         head_width=0.1, head_length=0.1, fc=color, ec=color,\n",
    "                         width=0.03)\n",
    "    \n",
    "    axes[1].set_aspect('equal')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title('Circle â†’ Ellipse')\n",
    "    \n",
    "    # Auto-scale\n",
    "    max_coord = np.max(np.abs(circle_transformed)) * 1.1\n",
    "    axes[1].set_xlim(-max_coord, max_coord)\n",
    "    axes[1].set_ylim(-max_coord, max_coord)\n",
    "    \n",
    "    # Plot 3: Eigenvalue spectrum\n",
    "    real_eigenvalues = np.real(eigenvalues)\n",
    "    axes[2].bar(range(len(real_eigenvalues)), real_eigenvalues, color=colors[:len(real_eigenvalues)])\n",
    "    axes[2].set_xlabel('Eigenvalue Index')\n",
    "    axes[2].set_ylabel('Eigenvalue')\n",
    "    axes[2].set_title('Eigenvalue Spectrum')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{title}\\nMatrix: {matrix}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"Eigenvalues: {eigenvalues}\")\n",
    "    print(f\"Eigenvectors (columns):\")\n",
    "    print(eigenvectors)\n",
    "    \n",
    "    # Verify Av = Î»v\n",
    "    print(\"\\nVerification (Av = Î»v):\")\n",
    "    for i, (val, vec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "        if np.isreal(val) and np.isreal(vec).all():\n",
    "            Av = matrix @ vec\n",
    "            lambda_v = val * vec\n",
    "            error = np.linalg.norm(Av - lambda_v)\n",
    "            print(f\"Eigenvector {i+1}: ||Av - Î»v|| = {error:.2e}\")\n",
    "\n",
    "# Test with different matrices\n",
    "print(\"=== Symmetric Matrix (Real Eigenvalues) ===\")\n",
    "symmetric = np.array([[3, 1],\n",
    "                      [1, 3]])\n",
    "visualize_eigenvectors(symmetric, \"Symmetric Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Covariance-like Matrix ===\")\n",
    "cov_matrix = np.array([[2, 1.5],\n",
    "                       [1.5, 1]])\n",
    "visualize_eigenvectors(cov_matrix, \"Covariance-like Matrix\")\n",
    "\n",
    "print(\"\\n=== Non-symmetric Matrix ===\")\n",
    "nonsymmetric = np.array([[2, 3],\n",
    "                         [1, 0]])\n",
    "visualize_eigenvectors(nonsymmetric, \"Non-symmetric Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA finds the principal directions of variance in data using eigenanalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our custom PCA implementation\n",
    "exec(open('../code/01_pca.py').read())\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Create correlated data\n",
    "mean = [2, 3]\n",
    "cov = [[2, 1.5], [1.5, 1.5]]\n",
    "data = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "# Interactive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Original data with principal components\n",
    "axes[0, 0].scatter(data[:, 0], data[:, 1], alpha=0.6, s=30)\n",
    "\n",
    "# Draw principal components\n",
    "mean_point = pca.mean_\n",
    "colors = ['red', 'blue']\n",
    "for i, (component, variance, color) in enumerate(zip(pca.components_, pca.explained_variance_, colors)):\n",
    "    # Scale by square root of eigenvalue\n",
    "    scale = 2 * np.sqrt(variance)\n",
    "    axes[0, 0].arrow(mean_point[0], mean_point[1], \n",
    "                     scale * component[0], scale * component[1],\n",
    "                     head_width=0.1, head_length=0.1, \n",
    "                     fc=color, ec=color, width=0.03,\n",
    "                     label=f'PC{i+1} (Î»={variance:.2f})')\n",
    "\n",
    "axes[0, 0].set_title('Original Data with Principal Components')\n",
    "axes[0, 0].set_xlabel('Feature 1')\n",
    "axes[0, 0].set_ylabel('Feature 2')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axis('equal')\n",
    "\n",
    "# Plot 2: Data in PC space\n",
    "axes[0, 1].scatter(data_pca[:, 0], data_pca[:, 1], alpha=0.6, s=30)\n",
    "axes[0, 1].set_title('Data in Principal Component Space')\n",
    "axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axis('equal')\n",
    "\n",
    "# Plot 3: Explained variance\n",
    "axes[1, 0].bar(['PC1', 'PC2'], pca.explained_variance_ratio_, color=colors)\n",
    "axes[1, 0].set_title('Explained Variance Ratio')\n",
    "axes[1, 0].set_ylabel('Variance Ratio')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    axes[1, 0].text(i, ratio + 0.01, f'{ratio:.1%}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: 1D projection (dimensionality reduction)\n",
    "pca_1d = PCA(n_components=1)\n",
    "data_1d = pca_1d.fit_transform(data)\n",
    "data_reconstructed = pca_1d.inverse_transform(data_1d)\n",
    "\n",
    "axes[1, 1].scatter(data[:, 0], data[:, 1], alpha=0.4, s=20, label='Original')\n",
    "axes[1, 1].scatter(data_reconstructed[:, 0], data_reconstructed[:, 1], \n",
    "                   alpha=0.8, s=20, label='1D Projection')\n",
    "\n",
    "# Draw projection lines for some points\n",
    "for i in range(0, len(data), 20):\n",
    "    axes[1, 1].plot([data[i, 0], data_reconstructed[i, 0]], \n",
    "                    [data[i, 1], data_reconstructed[i, 1]], \n",
    "                    'k-', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "axes[1, 1].set_title(f'1D Reconstruction ({pca_1d.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[1, 1].set_xlabel('Feature 1')\n",
    "axes[1, 1].set_ylabel('Feature 2')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative variance: {np.cumsum(pca.explained_variance_ratio_)}\")\n",
    "print(f\"Principal components (rows):\")\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on Real Data: Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Classes: {target_names}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca_iris = PCA()\n",
    "X_pca = pca_iris.fit_transform(X_scaled)\n",
    "\n",
    "# Create interactive 3D plot\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, (target, color, name) in enumerate(zip(np.unique(y), colors, target_names)):\n",
    "    mask = y == target\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=X_pca[mask, 0],\n",
    "        y=X_pca[mask, 1], \n",
    "        z=X_pca[mask, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=color),\n",
    "        name=name,\n",
    "        text=[f'{name}<br>PC1: {x:.2f}<br>PC2: {y:.2f}<br>PC3: {z:.2f}' \n",
    "              for x, y, z in zip(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2])],\n",
    "        hovertemplate='%{text}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Iris Dataset in 3D PCA Space',\n",
    "    scene=dict(\n",
    "        xaxis_title=f'PC1 ({pca_iris.explained_variance_ratio_[0]:.1%} variance)',\n",
    "        yaxis_title=f'PC2 ({pca_iris.explained_variance_ratio_[1]:.1%} variance)',\n",
    "        zaxis_title=f'PC3 ({pca_iris.explained_variance_ratio_[2]:.1%} variance)'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nExplained variance by component:\")\n",
    "for i, ratio in enumerate(pca_iris.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {ratio:.3f} ({ratio*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative explained variance:\")\n",
    "cumsum = np.cumsum(pca_iris.explained_variance_ratio_)\n",
    "for i, cum_ratio in enumerate(cumsum):\n",
    "    print(f\"  PC1-{i+1}: {cum_ratio:.3f} ({cum_ratio*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Regression with Normal Equation\n",
    "\n",
    "Let's implement linear regression using the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our linear regression implementation\n",
    "exec(open('../code/01_linear_regression.py').read())\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.randn(n_samples, 2)\n",
    "true_weights = np.array([3.0, -2.0])\n",
    "true_intercept = 1.5\n",
    "noise = 0.3 * np.random.randn(n_samples)\n",
    "y = X @ true_weights + true_intercept + noise\n",
    "\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True intercept: {true_intercept}\")\n",
    "print(f\"Noise std: 0.3\")\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegressionNormalEquation()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"\\nFitted weights: {model.weights}\")\n",
    "print(f\"Fitted intercept: {model.intercept:.3f}\")\n",
    "print(f\"RÂ² score: {model.score(X, y):.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "axes[0, 0].scatter(y, y_pred, alpha=0.6)\n",
    "min_val, max_val = min(y.min(), y_pred.min()), max(y.max(), y_pred.max())\n",
    "axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual')\n",
    "axes[0, 0].set_ylabel('Predicted')\n",
    "axes[0, 0].set_title(f'Actual vs Predicted (RÂ² = {model.score(X, y):.3f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Feature 1 relationship\n",
    "axes[0, 1].scatter(X[:, 0], y, alpha=0.6, label='Data')\n",
    "x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x2_mean = np.mean(X[:, 1])\n",
    "X_viz = np.column_stack([x1_range, np.full_like(x1_range, x2_mean)])\n",
    "y_viz = model.predict(X_viz)\n",
    "axes[0, 1].plot(x1_range, y_viz, 'r-', lw=2, label='Fitted line')\n",
    "axes[0, 1].set_xlabel('Feature 1')\n",
    "axes[0, 1].set_ylabel('Target')\n",
    "axes[0, 1].set_title('Feature 1 vs Target (Feature 2 at mean)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Feature 2 relationship\n",
    "axes[1, 0].scatter(X[:, 1], y, alpha=0.6, label='Data')\n",
    "x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "x1_mean = np.mean(X[:, 0])\n",
    "X_viz2 = np.column_stack([np.full_like(x2_range, x1_mean), x2_range])\n",
    "y_viz2 = model.predict(X_viz2)\n",
    "axes[1, 0].plot(x2_range, y_viz2, 'r-', lw=2, label='Fitted line')\n",
    "axes[1, 0].set_xlabel('Feature 2')\n",
    "axes[1, 0].set_ylabel('Target')\n",
    "axes[1, 0].set_title('Feature 2 vs Target (Feature 1 at mean)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Residuals\n",
    "residuals = y - y_pred\n",
    "axes[1, 1].scatter(y_pred, residuals, alpha=0.6)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Predicted')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title('Residual Plot')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the normal equation calculation step by step\n",
    "print(\"\\n=== Normal Equation Steps ===\")\n",
    "X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n",
    "print(f\"1. X with bias shape: {X_with_bias.shape}\")\n",
    "\n",
    "XtX = X_with_bias.T @ X_with_bias\n",
    "print(f\"2. X^T X shape: {XtX.shape}\")\n",
    "print(f\"   X^T X condition number: {np.linalg.cond(XtX):.2e}\")\n",
    "\n",
    "Xty = X_with_bias.T @ y\n",
    "print(f\"3. X^T y shape: {Xty.shape}\")\n",
    "\n",
    "beta = np.linalg.solve(XtX, Xty)  # More stable than computing inverse\n",
    "print(f\"4. Î² = (X^T X)^(-1) X^T y: {beta}\")\n",
    "print(f\"   Intercept: {beta[0]:.3f}\")\n",
    "print(f\"   Weights: {beta[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural Networks as Matrix Operations\n",
    "\n",
    "Let's see how neural networks are fundamentally just sequences of matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A simple 2-layer neural network to demonstrate matrix operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, X, verbose=False):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Input X shape: {X.shape}\")\n",
    "            print(f\"W1 shape: {self.W1.shape}, b1 shape: {self.b1.shape}\")\n",
    "        \n",
    "        # Layer 1: X @ W1 + b1\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"z1 = X @ W1 + b1, shape: {self.z1.shape}\")\n",
    "            print(f\"a1 = ReLU(z1), shape: {self.a1.shape}\")\n",
    "            print(f\"W2 shape: {self.W2.shape}, b2 shape: {self.b2.shape}\")\n",
    "        \n",
    "        # Layer 2: a1 @ W2 + b2\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"z2 = a1 @ W2 + b2, shape: {self.z2.shape}\")\n",
    "            print(f\"a2 = Sigmoid(z2), shape: {self.a2.shape}\")\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def visualize_weights(self):\n",
    "        \"\"\"Visualize the weight matrices.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Plot W1\n",
    "        im1 = axes[0].imshow(self.W1.T, cmap='RdBu', aspect='auto')\n",
    "        axes[0].set_title(f'W1 (Input â†’ Hidden)\\nShape: {self.W1.shape}')\n",
    "        axes[0].set_xlabel('Input Units')\n",
    "        axes[0].set_ylabel('Hidden Units')\n",
    "        plt.colorbar(im1, ax=axes[0])\n",
    "        \n",
    "        # Plot W2\n",
    "        im2 = axes[1].imshow(self.W2.T, cmap='RdBu', aspect='auto')\n",
    "        axes[1].set_title(f'W2 (Hidden â†’ Output)\\nShape: {self.W2.shape}')\n",
    "        axes[1].set_xlabel('Hidden Units')\n",
    "        axes[1].set_ylabel('Output Units')\n",
    "        plt.colorbar(im2, ax=axes[1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create a simple neural network\n",
    "np.random.seed(42)\n",
    "nn = SimpleNeuralNetwork(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Create sample input\n",
    "X_sample = np.array([[1.0, 0.5, -0.2],\n",
    "                     [0.3, -1.0, 0.8],\n",
    "                     [-0.5, 0.0, 1.2]])\n",
    "\n",
    "print(\"=== Neural Network Forward Pass ===\")\n",
    "output = nn.forward(X_sample, verbose=True)\n",
    "print(f\"\\nFinal output: {output}\")\n",
    "\n",
    "# Visualize the weight matrices\n",
    "nn.visualize_weights()\n",
    "\n",
    "# Show how different inputs activate different patterns\n",
    "test_inputs = [\n",
    "    np.array([[1, 0, 0]]),   # Unit vector in first dimension\n",
    "    np.array([[0, 1, 0]]),   # Unit vector in second dimension\n",
    "    np.array([[0, 0, 1]]),   # Unit vector in third dimension\n",
    "    np.array([[1, 1, 1]]),   # All ones\n",
    "    np.array([[-1, -1, -1]]) # All negative ones\n",
    "]\n",
    "\n",
    "print(\"\\n=== Activation Patterns for Different Inputs ===\")\n",
    "activations = []\n",
    "for i, test_input in enumerate(test_inputs):\n",
    "    output = nn.forward(test_input)\n",
    "    activations.append(nn.a1[0])  # Hidden layer activations\n",
    "    print(f\"Input {test_input[0]} â†’ Hidden: {nn.a1[0]} â†’ Output: {output[0]}\")\n",
    "\n",
    "# Visualize activation patterns\n",
    "activations = np.array(activations)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(activations.T, cmap='RdYlBu', aspect='auto')\n",
    "plt.colorbar(label='Activation Level')\n",
    "plt.xlabel('Input Pattern')\n",
    "plt.ylabel('Hidden Unit')\n",
    "plt.title('Hidden Layer Activation Patterns')\n",
    "plt.xticks(range(len(test_inputs)), \n",
    "           ['[1,0,0]', '[0,1,0]', '[0,0,1]', '[1,1,1]', '[-1,-1,-1]'])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Key Insights ===\")\n",
    "print(\"1. Each layer is just a matrix multiplication followed by non-linearity\")\n",
    "print(\"2. The weight matrices encode learned transformations\")\n",
    "print(\"3. Different inputs create different activation patterns\")\n",
    "print(\"4. The hidden layer learns to represent useful features\")\n",
    "print(\"5. Linear algebra makes this all computationally efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "Through this interactive exploration, you've seen how:\n",
    "\n",
    "1. **Vectors represent data points and directions** in high-dimensional spaces\n",
    "2. **Matrices encode transformations** that can rotate, scale, and shear\n",
    "3. **Eigenvalues/eigenvectors reveal fundamental properties** of these transformations\n",
    "4. **PCA finds principal directions of variance** using eigenanalysis\n",
    "5. **Linear regression uses matrix operations** for closed-form solutions\n",
    "6. **Neural networks are compositions** of linear transformations and non-linearities\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "Ready to dive deeper? Continue with:\n",
    "- **[Step 2: Calculus & Gradients](02_calculus_gradients.md)** - Learn how models actually learn through optimization\n",
    "- **Exercises**: Complete the [Linear Algebra Exercises](../exercises/01_linear_algebra_solutions.md)\n",
    "- **Code**: Explore the [implementation details](../code/) for deeper understanding\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- **3Blue1Brown**: \"Essence of Linear Algebra\" video series\n",
    "- **Khan Academy**: Linear Algebra course\n",
    "- **MIT 18.06**: Gilbert Strang's Linear Algebra lectures\n",
    "\n",
    "---\n",
    "\n",
    "*\"Linear algebra is the foundation upon which the entire edifice of machine learning rests.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}